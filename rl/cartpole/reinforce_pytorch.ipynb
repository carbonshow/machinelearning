{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdfe8c58f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' \n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dim: (4,), n_actions: 2\n"
     ]
    }
   ],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "print(\"state_dim: {}, n_actions: {}\".format(state_dim, n_actions))\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(state_dim[0], 100),\n",
    "  nn.Linear(100, n_actions),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    with torch.no_grad():\n",
    "        probs = model(torch.tensor(states).float())\n",
    "        return nn.functional.softmax(probs, -1).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5505417  0.4494583 ]\n",
      " [0.54956746 0.45043257]\n",
      " [0.54601747 0.45398256]\n",
      " [0.55139893 0.44860113]\n",
      " [0.55526507 0.444735  ]]\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "print(test_probas)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(range(n_actions), 1, p=action_probs)[0]\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    g = np.zeros(len(rewards))\n",
    "    g[-1] = rewards[-1]\n",
    "    for t in reversed(range(0, len(rewards)-1)):\n",
    "        g[t] = rewards[t] + gamma*g[t+1]\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    loss = entropy*entropy_coef\n",
    "\n",
    "    # Gradient descent step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:19.060\n",
      "mean reward:29.520\n",
      "mean reward:49.710\n",
      "mean reward:48.430\n",
      "mean reward:69.090\n",
      "mean reward:166.250\n",
      "mean reward:211.560\n",
      "mean reward:244.720\n",
      "mean reward:121.360\n",
      "mean reward:206.610\n",
      "mean reward:285.170\n",
      "mean reward:300.290\n",
      "mean reward:124.810\n",
      "mean reward:153.400\n",
      "mean reward:854.270\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n",
      "\u001b[31mERROR: VideoRecorder encoder exited with status 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.0.31228.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
